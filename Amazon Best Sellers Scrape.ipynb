{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import system\n",
    "from math import floor\n",
    "from copy import deepcopy\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapes the best seller URLs from Amazon.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_sellers_url():\n",
    "    #webdriver to amazon best seller page \n",
    "    pd.set_option('display.max_columns', 20)\n",
    "    pd.set_option('display.max_colwidth', 200)\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get('https://www.amazon.com/Best-Sellers/zgbs')\n",
    "    \n",
    "    best_sellers_total = []\n",
    "    for i in range(40):\n",
    "        x_path = driver.find_element_by_xpath(f'//*[@id=\"zg_browseRoot\"]/ul/li[{i+1}]/a')\n",
    "        url = x_path.get_attribute('href')\n",
    "        best_sellers_total.append(url)\n",
    "        \n",
    "    driver.quit()\n",
    "        \n",
    "    return best_sellers_total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_urls = best_sellers_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Seller Item Scrape \n",
    "- Takes in the URL's from the above function \n",
    "- Scrapes the products from the amazon best seller lists \n",
    "- Scrapes URL's for each individual product to later feed into camelcamelcamel for price history\n",
    "- creates list of ASIN numbers which will be used as the primary IDs in the DB \n",
    "- scrapes the ratings \n",
    "- names of the items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_sellers_scrape(url):\n",
    "    #webdriver to amazon best seller page \n",
    "    pd.set_option('display.max_columns', 20)\n",
    "    pd.set_option('display.max_colwidth', 200)\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    #final Url list: outputs urls that will go to camelcamelcamel\n",
    "    final_urls = []\n",
    "    #will create list for primary keys \n",
    "    id_list = []\n",
    "    #scrapes the name, ratings, reviews and price\n",
    "    ratings = []\n",
    "    item = driver.find_element_by_xpath('//*[@id=\"zg-center-div\"]')\n",
    "    items = item.text.split('#')\n",
    "    final_list = []\n",
    "#     [re.findall(r'\\n(.*)',i)[:3] for i in items if re.findall(r'\\n(.*)',i) != None and len(re.findall(r'\\n(.*)',i))!= 0]\n",
    "    for i in items:\n",
    "        try:\n",
    "            string_text = re.findall(r'\\n(.*)',i)\n",
    "            if re.findall(r'\\n(.*)',i) != None and len(re.findall(r'\\n(.*)',i))!= 0 :\n",
    "                final_list.append(string_text[:3])\n",
    "            for z in final_list:\n",
    "                if z[2] == '':\n",
    "                    z[2] = string_text[1]\n",
    "                    z[1] = '000'\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    link_list = [i.get_attribute('href') for i in driver.find_elements_by_class_name('a-link-normal')]\n",
    "    \n",
    "    for i in range(1,51):\n",
    "        try:\n",
    "            if driver.find_element_by_xpath(f'//*[@id=\"zg-ordered-list\"]/li[{i}]/span/div/span/div[1]/a[1]'):\n",
    "                rating = driver.find_element_by_xpath(f'//*[@id=\"zg-ordered-list\"]/li[{i}]/span/div/span/div[1]/a[1]')\n",
    "                ratings.append(rating.get_attribute('title'))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "    #trys going to the second page if there is one.  If there are no best sellers it moves to the next page\n",
    "    try:\n",
    "        page_2 = driver.find_element_by_xpath('//*[@id=\"zg-center-div\"]/div[2]/div/ul/li[3]/a')\n",
    "    except:\n",
    "        pass \n",
    "    try:\n",
    "        page_2 = driver.find_element_by_class_name('a-normal')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        page_2.click()\n",
    "    except:\n",
    "        pass\n",
    "   \n",
    "    \n",
    "    time.sleep(1.5)\n",
    "    #scrapes the name, ratings, reviews and price for the second page \n",
    "    item = driver.find_element_by_xpath('//*[@id=\"zg-center-div\"]')\n",
    "    items = item.text.split('#')\n",
    "    final_list1 = []\n",
    "    \n",
    "    for i in items:\n",
    "        try:\n",
    "            string_text = re.findall(r'\\n(.*)',i)\n",
    "            if re.findall(r'\\n(.*)',i) != None and len(re.findall(r'\\n(.*)',i))!= 0 :\n",
    "                final_list1.append(string_text[:3])\n",
    "            for z in final_list1:\n",
    "                if z[2] == '':\n",
    "                    z[2] = string_text[1]\n",
    "                    z[1] = '000'\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    \n",
    "    link_list1 = [i.get_attribute('href') for i in driver.find_elements_by_class_name('a-link-normal')]\n",
    "    #cleans up the links and appends them to a final url list \n",
    "    for i in link_list + link_list1:\n",
    "        a = str(i)\n",
    "        if a.find('reviews') == -1 and a not in final_urls and a.find('gp') == -1 and len(a) != 0:\n",
    "            final_urls.append(a)\n",
    "            \n",
    "    #gets ids from urls and puts them in list id_list\n",
    "    for z in final_urls:\n",
    "        for i in range(len(z)-1):\n",
    "            if z[i:i+4] == '/dp/':\n",
    "                id_list.append(z[i+4:i+14])\n",
    "                \n",
    "    for i in range(1,51):\n",
    "        try:\n",
    "            rating = driver.find_element_by_xpath(f'//*[@id=\"zg-ordered-list\"]/li[{i}]/span/div/span/div[1]/a[1]')\n",
    "            ratings.append(rating.get_attribute('title'))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    driver.quit()\n",
    "    return final_list + final_list1, final_urls, id_list, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "info, urls, ids, ratings = best_sellers_scrape(best_urls[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip_lists function takes all of the lists returned in the best_seller_scrape function and combines info + urls + ids + ratings into one list for each item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_lists(info, urls, ids, ratings):\n",
    "    new_list = []\n",
    "    final_list = []\n",
    "    for i in range(len(info)):\n",
    "        new_list.append(info[i][0])\n",
    "        new_list.append(info[i][1])\n",
    "        new_list.append(info[i][2])\n",
    "        new_list.append(ids[i])\n",
    "        new_list.append(ratings[i][:3])\n",
    "        new_list.append(urls[i])\n",
    "        final_list.append(new_list)\n",
    "        new_list = []\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to get all the item info from each category into an organized format \n",
    "def scrape_all_zip():\n",
    "    listy = []\n",
    "    for i in range(len(best_urls)):\n",
    "        try:\n",
    "            info, urls, ids, ratings = best_sellers_scrape(best_urls[i])\n",
    "            listy.append(zip_lists(info, urls, ids, ratings))\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return listy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_item_data = scrape_all_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts all of the urls for each item into a list\n",
    "def get_urls_for_camel(amazon_item_data):\n",
    "    urls = []\n",
    "    for category in amazon_item_data:\n",
    "        for item in category:\n",
    "            urls.append(item[-1])\n",
    "            \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_urls = get_urls_for_camel(amazon_item_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### camel_info scrapes camelcamelcamel for price history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def camel_info(amazon_products):\n",
    "    pd.set_option('display.max_columns', 20)\n",
    "    pd.set_option('display.max_colwidth', 200)\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get('https://camelcamelcamel.com/')\n",
    "    time.sleep(2)\n",
    "    search = driver.find_element_by_xpath('//*[@id=\"sq\"]')\n",
    "    time.sleep(1.5)\n",
    "    search.click()\n",
    "#     time.sleep(3)\n",
    "    item_dict = {}\n",
    "    item_list = []\n",
    "    \n",
    "    for i in amazon_products: \n",
    "        try:\n",
    "            search = driver.find_element_by_xpath('//*[@id=\"sq\"]')\n",
    "            search.send_keys(i)\n",
    "\n",
    "            search_button = driver.find_element_by_xpath('//*[@id=\"sqbtn\"]')\n",
    "            search_button.click()\n",
    "            time.sleep(3)\n",
    "            \n",
    "            try:\n",
    "                skip = driver.find_element_by_xpath('//*[@id=\"content\"]/p[1]')\n",
    "                if skip.text[0] == 'S':\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "\n",
    "            try:\n",
    "                current_price = driver.find_element_by_xpath('//*[@id=\"histories\"]/div[1]/div/div[2]/div/div/table/tbody/tr[1]')\n",
    "                current_list = current_price.text.split(' ')\n",
    "                item_dict['current_price'] = current_list[1]\n",
    "                item_dict['current_date'] = current_list[2] + \" \" + current_list[3] + \" \" + current_list[4]\n",
    "                highest_price = driver.find_element_by_xpath('//*[@id=\"histories\"]/div[1]/div/div[2]/div/div/table/tbody/tr[2]')\n",
    "                highest_list = highest_price.text.split(' ')\n",
    "                item_dict['highest_price'] = highest_list[2]\n",
    "                item_dict['highest_date'] = highest_date = highest_list[3] + \" \" + highest_list[4] + \" \" + highest_list[5]\n",
    "                lowest_price = driver.find_element_by_xpath('//*[@id=\"histories\"]/div[1]/div/div[2]/div/div/table/tbody/tr[3]')\n",
    "                lowest_list = lowest_price.text.split(' ')\n",
    "                item_dict['lowest_price'] = lowest_list[2]\n",
    "                item_dict['lowest_date'] = lowest_list[3] + \" \" + lowest_list[4] + \" \" + lowest_list[5]\n",
    "                asin = driver.find_element_by_xpath('//*[@id=\"content\"]/div[6]/div/table/tbody/tr[9]/td[2]')\n",
    "                item_dict['asin'] = asin.text\n",
    "                category = driver.find_element_by_xpath('//*[@id=\"content\"]/div[6]/div/table/tbody/tr[2]/td[2]/a')\n",
    "                item_dict['category'] = category.text\n",
    "                item_list.append(item_dict)\n",
    "\n",
    "                item_dict = {}\n",
    "                time.sleep(.5)\n",
    "            except: \n",
    "                item_dict['current_price'] = \"\"\n",
    "                item_dict['current_date'] = \"\"\n",
    "                item_dict['highest_price'] = \"\"\n",
    "                item_dict['highest_date'] = \"\"\n",
    "                item_dict['lowest_price'] = \"\"\n",
    "                item_dict['lowest_date'] = \"\"\n",
    "                asin = driver.find_element_by_xpath('//*[@id=\"content\"]/div[6]/div/table/tbody/tr[9]/td[2]')\n",
    "                item_dict['asin'] = asin.text\n",
    "                try:\n",
    "                    category = driver.find_element_by_xpath('//*[@id=\"content\"]/div[6]/div/table/tbody/tr[2]/td[2]/a')\n",
    "                    item_dict['category'] = category.text\n",
    "                except: \n",
    "                    category = driver.find_element_by_xpath('//*[@id=\"content\"]/div[6]/div/table/tbody/tr[1]/td[2]/a')\n",
    "                    item_dict['category'] = category.text\n",
    "                item_list.append(item_dict)\n",
    "                \n",
    "        except:\n",
    "            continue\n",
    "    driver.quit()\n",
    "    return item_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "camel_list = camel_info(amazon_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts item information into a format that can be commited to sql db\n",
    "def get_items_for_camel(amazon_item_data):\n",
    "    items = []\n",
    "    for category in amazon_item_data:\n",
    "        for item in category:\n",
    "            items.append(item)\n",
    "            \n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_items = get_items_for_camel(amazon_item_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_items_tuple = [tuple(i) for i in amazon_items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_sql(camel_query):\n",
    "    camel_list = []\n",
    "    for i in camel_query:\n",
    "        camel_tuple = (i['current_price'], i['current_date'], i['highest_price'], i['highest_date'], i['lowest_price'], i['lowest_date'], i['asin'], i['category'])\n",
    "        camel_list.append(camel_tuple)\n",
    "    return camel_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = camel_sql(camel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'amazon'\n",
    "cnx = mysql.connector.connect(\n",
    "    host = config.host,\n",
    "    user = config.user,\n",
    "    password = config.password,\n",
    "    database = db_name\n",
    ")\n",
    "print(cnx)\n",
    "cursor = cnx.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_camels = \"\"\"\n",
    "CREATE TABLE camels (\n",
    "asin TEXT NOT NULL,\n",
    "category TEXT NOT NULL,\n",
    "cprice TEXT,\n",
    "cdate TEXT,\n",
    "hprice TEXT,\n",
    "hdate TEXT,\n",
    "lprice TEXT,\n",
    "ldate TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(create_camels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commit_camels_sql(final_results)\n",
    "    stmt = \"INSERT INTO camels (asin, category, cprice, cdate, hprice, hdate, lprice, ldate) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "    cursor.executemany(stmt, final_results)\n",
    "    cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_query = \"\"\"CREATE TABLE amazon(\n",
    "name TEXT,\n",
    "num_reviews TEXT,\n",
    "price TEXT,\n",
    "product_id TEXT,\n",
    "rating TEXT,\n",
    "url TEXT\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(amazon_query)\n",
    "cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_amazon(amazon_items_tuple):\n",
    "    insert_amazon = \"\"\"INSERT INTO amazon(name,num_reviews, price, product_id, rating, url)\n",
    "    VALUES (%s,%s,%s,%s,%s,%s)\"\"\"\n",
    "    cursor.executemany(insert_amazon, amazon_items_tuple)\n",
    "    cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
